{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3747:>(6 + 2) / 10][Stage 3808:>(0 + 0) / 10][Stage 3827:> (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MovieLens Exploration\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data\n",
    "ratings_df = spark.read.csv(\"../data/ratings_small.csv\", header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv(\"../data/movies_metadata.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "ratings_df.show()\n",
    "\n",
    "# Print the schema of the dataframe\n",
    "ratings_df.printSchema()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Average rating per movie\n",
    "avg_ratings = ratings_df.groupBy(\"movieId\").agg(F.avg(\"rating\").alias(\"average_rating\"))\n",
    "avg_ratings.show()\n",
    "\n",
    "# Number of ratings per user\n",
    "user_ratings_count = ratings_df.groupBy(\"userId\").agg(F.count(\"rating\").alias(\"num_ratings\"))\n",
    "user_ratings_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.][Stage 3827:> (0 + 0) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/solaraeth/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solaraeth/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/solaraeth/anaconda3/envs/MLRecommender/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Data Preprocessing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# 1. Handle Missing Values\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m ratings_df\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m----> 5\u001b[0m     \u001b[39mprint\u001b[39m(col, \u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwith null values: \u001b[39m\u001b[39m\"\u001b[39m, ratings_df\u001b[39m.\u001b[39mfilter(ratings_df[col]\u001b[39m.\u001b[39misNull())\u001b[39m.\u001b[39mcount())\n\u001b[1;32m      7\u001b[0m \u001b[39m# Drop them if necessary (just an example)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# ratings_df = ratings_df.dropna()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[39m# Join DataFrames to get movie titles in ratings_df\u001b[39;00m\n\u001b[1;32m     15\u001b[0m ratings_df \u001b[39m=\u001b[39m ratings_df\u001b[39m.\u001b[39mjoin(movies_df, [\u001b[39m\"\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mselect(ratings_df[\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m], movies_df[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mcount())\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m   1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream\u001b[39m.\u001b[39mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "# 1. Handle Missing Values\n",
    "for col in ratings_df.columns:\n",
    "    print(col, \"\\t\", \"with null values: \", ratings_df.filter(ratings_df[col].isNull()).count())\n",
    "\n",
    "# Drop them if necessary (just an example)\n",
    "# ratings_df = ratings_df.dropna()\n",
    "\n",
    "# 2. Convert Data Types if necessary\n",
    "# As an example, converting 'rating' column to float type\n",
    "# ratings_df = ratings_df.withColumn(\"rating\", ratings_df[\"rating\"].cast(\"float\"))\n",
    "\n",
    "# Join DataFrames to get movie titles in ratings_df\n",
    "ratings_df = ratings_df.join(movies_df, [\"movieId\"], \"left\").select(ratings_df[\"*\"], movies_df[\"title\"])\n",
    "\n",
    "# 3. Split the Dataset\n",
    "train_data, test_data = ratings_df.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "# Check the count of each dataset\n",
    "print(\"Number of training records: \" + str(train_data.count()))\n",
    "print(\"Number of testing records : \" + str(test_data.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/31 05:53:29 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "# Define the ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative=True, implicitPrefs=False, coldStartStrategy=\"drop\")\n",
    "\n",
    "# Set model parameters\n",
    "model = als.fit(train_data)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.9101083617002579\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Evaluate the model with Root Mean Squared Error\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/31 19:45:45 WARN CacheManager: Asked to cache already cached data.\n",
      "23/08/31 19:45:45 WARN CacheManager: Asked to cache already cached data.\n",
      "[Stage 2734:=========>     (6 + 2) / 10][Stage 2744:>               (0 + 0) / 1]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m crossval \u001b[39m=\u001b[39m CrossValidator(estimator\u001b[39m=\u001b[39mals,\n\u001b[1;32m     18\u001b[0m                           estimatorParamMaps\u001b[39m=\u001b[39mParamGrid,\n\u001b[1;32m     19\u001b[0m                           evaluator\u001b[39m=\u001b[39mevaluator,\n\u001b[1;32m     20\u001b[0m                           numFolds\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[39m# Run cross-validation\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m cvModel \u001b[39m=\u001b[39m crossval\u001b[39m.\u001b[39mfit(train_data)\n\u001b[1;32m     25\u001b[0m \u001b[39m# Use test set to measure the accuracy of our model on the new data\u001b[39;00m\n\u001b[1;32m     26\u001b[0m predictions \u001b[39m=\u001b[39m cvModel\u001b[39m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[39m=\u001b[39m datasets[i][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[39mfor\u001b[39;00m j, metric, subModel \u001b[39min\u001b[39;00m pool\u001b[39m.\u001b[39mimap_unordered(\u001b[39mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[39m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[39mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 861\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cond\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    862\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_items\u001b[39m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2734:============>  (8 + 2) / 10][Stage 2744:>               (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Define the ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\" , nonnegative=True, implicitPrefs=False, coldStartStrategy=\"drop\")\n",
    "\n",
    "# Define the evaluator as RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "# Construct the grid of parameters to search over\n",
    "ParamGrid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [5, 10, 25]) \\\n",
    "    .addGrid(als.maxIter, [5, 10]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1,]) \\\n",
    "    .build()\n",
    "\n",
    "# Setup cross valudation\n",
    "crossval = CrossValidator(estimator=als,\n",
    "                          estimatorParamMaps=ParamGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=2)\n",
    "\n",
    "# Run cross-validation\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Use test set to measure the accuracy of our model on the new data\n",
    "predictions = cvModel.transform(test_data)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Best model RMSE: \", rmse)\n",
    "print(\"Best model rank: \", cvModel.bestModel.rank)\n",
    "print(\"Best model maxIter: \", cvModel.bestModel._java_obj.parent().getMaxIter())\n",
    "print(\"Best model regParam: \", cvModel.bestModel._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3579:>(8 + 2) / 10][Stage 3594:> (0 + 0) / 1][Stage 3597:> (0 + 0) / 1]0]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|     1|[{2563, 4.248676}...|\n",
      "|     2|[{92494, 5.902215...|\n",
      "|     3|[{92494, 5.579682...|\n",
      "|     4|[{83411, 7.021948...|\n",
      "|     5|[{83411, 5.636155...|\n",
      "|     6|[{83411, 5.321218...|\n",
      "|     7|[{83411, 5.557587...|\n",
      "|     8|[{83411, 5.659895...|\n",
      "|     9|[{92494, 5.805102...|\n",
      "|    10|[{92494, 6.19543}...|\n",
      "|    11|[{92494, 6.099708...|\n",
      "|    12|[{6219, 5.4512215...|\n",
      "|    13|[{90061, 5.158160...|\n",
      "|    14|[{3414, 5.1059365...|\n",
      "|    15|[{5071, 4.9651093...|\n",
      "|    16|[{83411, 5.749088...|\n",
      "|    17|[{83411, 5.813849...|\n",
      "|    18|[{83411, 5.523983...|\n",
      "|    19|[{83411, 5.783284...|\n",
      "|    20|[{121231, 5.60584...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3613:>(2 + 2) / 10][Stage 3623:> (0 + 0) / 1][Stage 3637:>(0 + 0) / 10]0]\r"
     ]
    }
   ],
   "source": [
    "# Retrain the model using the best hyperparameters\n",
    "best_als = ALS(\n",
    "    rank=5,\n",
    "    maxIter=5,\n",
    "    regParam=0.1,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative=True,\n",
    "    implicitPrefs=False,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "final_model = best_als.fit(ratings_df)\n",
    "\n",
    "# Generate top 10 movie recommendations for each user\n",
    "user_recommendations = final_model.recommendForAllUsers(10)\n",
    "\n",
    "# Show recommendations for the first few users\n",
    "user_recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3615:>              (0 + 2) / 10][Stage 3647:>               (0 + 0) / 1]\r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_USING_COLUMN_FOR_JOIN] USING column `movieId` cannot be resolved on the right side of the join. The right-side columns: [`adult`, `belongs_to_collection`, `budget`, `genres`, `homepage`, `id`, `imdb_id`, `original_language`, `original_title`, `overview`, `popularity`, `poster_path`, `production_companies`, `production_countries`, `release_date`, `revenue`, `runtime`, `spoken_languages`, `status`, `tagline`, `title`, `video`, `vote_average`, `vote_count`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m movies_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mcsv(\u001b[39m\"\u001b[39m\u001b[39m../data/movies_metadata.csv\u001b[39m\u001b[39m\"\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, inferSchema\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[39m# Join the exploded recomendations with the movies_df to get the titles\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m final_recommendations \u001b[39m=\u001b[39m exploded_recs\u001b[39m.\u001b[39mjoin(movies_df, on\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mselect(\u001b[39m\"\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Show the recommendations\u001b[39;00m\n\u001b[1;32m     20\u001b[0m final_recommendations\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/pyspark/sql/dataframe.py:2344\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2342\u001b[0m         on \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jseq([])\n\u001b[1;32m   2343\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(how, \u001b[39mstr\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mhow should be a string\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2344\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mjoin(other\u001b[39m.\u001b[39m_jdf, on, how)\n\u001b[1;32m   2345\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/MLRecommender/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_USING_COLUMN_FOR_JOIN] USING column `movieId` cannot be resolved on the right side of the join. The right-side columns: [`adult`, `belongs_to_collection`, `budget`, `genres`, `homepage`, `id`, `imdb_id`, `original_language`, `original_title`, `overview`, `popularity`, `poster_path`, `production_companies`, `production_countries`, `release_date`, `revenue`, `runtime`, `spoken_languages`, `status`, `tagline`, `title`, `video`, `vote_average`, `vote_count`]."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3745:============>  (8 + 2) / 10][Stage 3807:=========>     (6 + 0) / 10]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Explode the recommendations column into seperate rows\n",
    "exploded_recs = user_recommendations.select(\"userId\", explode(\"recommendations\").alias(\"recommendation\"))\n",
    "\n",
    "# Extract movieId and rating from the recommendation struct column\n",
    "exploded_recs = exploded_recs.select(\n",
    "    \"userId\",\n",
    "    col(\"recommendation.movieId\").alias(\"movieId\"),\n",
    "    col(\"recommendation.rating\").alias(\"prediction\")\n",
    ")\n",
    "\n",
    "# Join the exploded recomendations with the movies_df to get the titles\n",
    "final_recommendations = exploded_recs.join(movies_df, on=\"movieId\").select(\"userId\", \"title\", \"prediction\")\n",
    "\n",
    "# Show the recommendations\n",
    "final_recommendations.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLRecommender",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
